<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Erik Wijmans">
  

  
  
  
  
    
      
    
  
  <meta name="description" content="We present Variable Experience Rollout (VER), a technique for efficiently scaling batched on-policy reinforcement learning in heterogenous environments (where different environments take vastly different times for generating rollouts) to many GPUs residing on, potentially, many machines. VER combines the strengths of and blurs the line between synchronous and asynchronous on-policy RL methods (SyncOnRL and AsyncOnRL, respectively). Specifically, it learns from on-policy experience (like SyncOnRL) and has no synchronization points (like AsyncOnRL), enabling high throughput. We find that VER leads to significant and consistent speed-ups across a broad range of embodied navigation and mobile manipulation tasks in photorealistic 3D simulation environments. Specifically, for PointGoal navigation and ObjectGoal navigation in Habitat 1.0, VER is 60-100% faster (1.6-2x speedup) than DD-PPO, the current state of art for distributed SyncOnRL, with similar sample efficiency. For mobile manipulation tasks (open fridge/cabinet, pick/place objects) in Habitat 2.0 VER is 150% faster (2.5x speedup) on 1 GPU and 170% faster (2.7x speedup) on 8 GPUs than DD-PPO. Compared to SampleFactory (the current state-of-the-art AsyncOnRL), VER matches its speed on 1 GPU, and is 70% faster (1.7x speedup) on 8 GPUs with better sample efficiency. We leverage these speed-ups to train chained skills for GeometricGoal rearrangement tasks in the Home Assistant Benchmark (HAB). We find a surprising emergence of navigation in skills that do not ostensible require any navigation. Specifically, the Pick skill involves a robot picking an object from a table. During training the robot was always spawned close to the table and never needed to navigate. However, we find that if base movement is part of the action space, the robot learns to navigate then pick an object in new environments with 50% success, demonstrating surprisingly high out-of-distribution generalization.">

  
  <link rel="alternate" hreflang="en-us" href="https://wijmans.xyz/publication/ver/">

  


  

  
  
  <meta name="theme-color" content="#2980b9">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha384-qpNsn9fNAkiBXwcwfxPTn1Ou6UU9P6pYGWQqLRIJOlPsAKOA+8XzN2coCnVnb8Fa" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="https://wijmans.xyz/index.xml" type="application/rss+xml" title="Erik Wijmans">
  <link rel="feed" href="https://wijmans.xyz/index.xml" type="application/rss+xml" title="Erik Wijmans">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://wijmans.xyz/publication/ver/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@erikwijmans">
  <meta property="twitter:creator" content="@erikwijmans">
  
  <meta property="og:site_name" content="Erik Wijmans">
  <meta property="og:url" content="https://wijmans.xyz/publication/ver/">
  <meta property="og:title" content="VER: Scaling On-Policy RL Leads to Emergence of Navigation in Embodied Rearrangement | Erik Wijmans">
  <meta property="og:description" content="We present Variable Experience Rollout (VER), a technique for efficiently scaling batched on-policy reinforcement learning in heterogenous environments (where different environments take vastly different times for generating rollouts) to many GPUs residing on, potentially, many machines. VER combines the strengths of and blurs the line between synchronous and asynchronous on-policy RL methods (SyncOnRL and AsyncOnRL, respectively). Specifically, it learns from on-policy experience (like SyncOnRL) and has no synchronization points (like AsyncOnRL), enabling high throughput. We find that VER leads to significant and consistent speed-ups across a broad range of embodied navigation and mobile manipulation tasks in photorealistic 3D simulation environments. Specifically, for PointGoal navigation and ObjectGoal navigation in Habitat 1.0, VER is 60-100% faster (1.6-2x speedup) than DD-PPO, the current state of art for distributed SyncOnRL, with similar sample efficiency. For mobile manipulation tasks (open fridge/cabinet, pick/place objects) in Habitat 2.0 VER is 150% faster (2.5x speedup) on 1 GPU and 170% faster (2.7x speedup) on 8 GPUs than DD-PPO. Compared to SampleFactory (the current state-of-the-art AsyncOnRL), VER matches its speed on 1 GPU, and is 70% faster (1.7x speedup) on 8 GPUs with better sample efficiency. We leverage these speed-ups to train chained skills for GeometricGoal rearrangement tasks in the Home Assistant Benchmark (HAB). We find a surprising emergence of navigation in skills that do not ostensible require any navigation. Specifically, the Pick skill involves a robot picking an object from a table. During training the robot was always spawned close to the table and never needed to navigate. However, we find that if base movement is part of the action space, the robot learns to navigate then pick an object in new environments with 50% success, demonstrating surprisingly high out-of-distribution generalization."><meta property="og:image" content="https://wijmans.xyz/img/ver-website-teaser.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2022-08-28T14:53:59-04:00">
  
  <meta property="article:modified_time" content="2022-08-28T14:53:59-04:00">
  

  

  <title>VER: Scaling On-Policy RL Leads to Emergence of Navigation in Embodied Rearrangement | Erik Wijmans</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Erik Wijmans</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#press">
            
            <span>Press</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#award">
            
            <span>Awards</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>

<div class="pub" itemscope itemtype="http://schema.org/CreativeWork">

  
<div class="article-header">
  <img src="/img/ver-website-teaser.jpg" class="article-banner" itemprop="image" style="max-width: 40%; height: auto; margin-left: auto; margin-right: auto;">
  
</div>



  <div class="article-container">
    <h1 itemprop="name">VER: Scaling On-Policy RL Leads to Emergence of Navigation in Embodied Rearrangement</h1>
    <span class="pub-authors" itemprop="author">
      
      <strong>Erik Wijmans</strong>, Irfan Essa, Dhruv Batra
      
    </span>
    <span class="pull-right">
      

    </span>

    

    
    <h3>Abstract</h3>
    <p class="pub-abstract" itemprop="text">We present Variable Experience Rollout (VER), a technique for efficiently scaling batched on-policy reinforcement learning in heterogenous environments (where different environments take vastly different times for generating rollouts) to many GPUs residing on, potentially, many machines. VER combines the strengths of and blurs the line between synchronous and asynchronous on-policy RL methods (SyncOnRL and AsyncOnRL, respectively). Specifically, it learns from on-policy experience (like SyncOnRL) and has no synchronization points (like AsyncOnRL), enabling high throughput. We find that VER leads to significant and consistent speed-ups across a broad range of embodied navigation and mobile manipulation tasks in photorealistic 3D simulation environments. Specifically, for PointGoal navigation and ObjectGoal navigation in Habitat 1.0, VER is 60-100% faster (1.6-2x speedup) than DD-PPO, the current state of art for distributed SyncOnRL, with similar sample efficiency. For mobile manipulation tasks (open fridge/cabinet, pick/place objects) in Habitat 2.0 VER is 150% faster (2.5x speedup) on 1 GPU and 170% faster (2.7x speedup) on 8 GPUs than DD-PPO. Compared to SampleFactory (the current state-of-the-art AsyncOnRL), VER matches its speed on 1 GPU, and is 70% faster (1.7x speedup) on 8 GPUs with better sample efficiency. We leverage these speed-ups to train chained skills for GeometricGoal rearrangement tasks in the Home Assistant Benchmark (HAB). We find a surprising emergence of navigation in skills that do not ostensible require any navigation. Specifically, the Pick skill involves a robot picking an object from a table. During training the robot was always spawned close to the table and never needed to navigate. However, we find that if base movement is part of the action space, the robot learns to navigate then pick an object in new environments with 50% success, demonstrating surprisingly high out-of-distribution generalization.</p>
    

    
    <div class="row">
      <div class="col-sm-1"></div>
      <div class="col-sm-10">
        <div class="row">
          <div class="col-xs-12 col-sm-3 pub-row-heading">Type</div>
          <div class="col-xs-12 col-sm-9">
            
            <a href="/publication/#1">
              Conference paper
            </a>
            
          </div>
        </div>
      </div>
      <div class="col-sm-1"></div>
    </div>
    <div class="visible-xs space-below"></div>
    

    
    <div class="row">
      <div class="col-sm-1"></div>
      <div class="col-sm-10">
        <div class="row">
          <div class="col-xs-12 col-sm-3 pub-row-heading">Publication</div>
          <div class="col-xs-12 col-sm-9">Neural Information Processing Systems (NeurIPS)</div>
        </div>
      </div>
      <div class="col-sm-1"></div>
    </div>
    <div class="visible-xs space-below"></div>
    


    <div class="row" style="padding-top: 10px">
      <div class="col-sm-1"></div>
      <div class="col-sm-10">
        <div class="row">
          <div class="col-xs-12 col-sm-3 pub-row-heading" style="line-height:34px;">Links</div>
          <div class="col-xs-12 col-sm-9">

            








<a class="btn btn-primary btn-outline" href="https://github.com/facebookresearch/habitat-api/tree/main/habitat_baselines/rl/ver" target="_blank" rel="noopener">
  Code
</a>














          </div>
        </div>
      </div>
      <div class="col-sm-1"></div>
    </div>
    <div class="visible-xs space-below"></div>

    <div class="space-below"></div>

    <div class="article-style">

<h1 id="introduction">Introduction</h1>

<p>How can we combine the benefit of synchronous (high sample efficiency) and asynchronous (high throughput) systems for batched on-policy reinforcement learning?</p>

<p><img src="/files/ver-post/ver-teaser-v4.jpg" alt="VER System Overview" /></p>

<p>Systems for batched on-policy reinforcement learning
collect experience from many (N) environments simultaneously using the policy and update it with this cumulative experience.
They are broadly divided into two classes: synchronous (Sync) and asynchronous (Aync).
Sync contains two synchronization points: first the policy is executed for the entire batch $(o_t \rightarrow a_t)^B_{b=1}$ (A), then actions are executed in <em>all</em> environments,
$(s_t, a_t \rightarrow s_{t+1}, o_{t+1})^B_{b=1}$ (B), until $T$ steps have been collected from all $N$ environments.
This $(T, N)$-shaped batch of experience is used to update the policy &copy;.
Synchronization reduces throughput as
the system spends significant (sometimes the most) time waiting for the slowest environment to finish.
This known as is the straggler effect.</p>

<p>Aync removes these synchronization points, thereby mitigating the straggler effect and improving throughput.
Actions are taken as soon as they are computed, $a_t \rightarrow o_{t+1}$ (D), the next action is computed as soon as the observation is ready, $o_t \rightarrow a_t$, and the policy is updated as soon as enough experience is collected (E).
However, Aync systems are not able to ensure that all experience has been collected by only the current policy and thus must consume <em>near</em>-policy data.
This reduces sample efficiency.
Thus, status quo leaves us with
 an unpleasant tradeoff &ndash; high sample-efficiency with low throughput or high throughput with low sample-efficiency.</p>

<p>In this work, we propose Variable Experience Rollout (VER).
VER
combines the strengths of and blurs the line between
Sync and Aync.
Like Sync, VER collects experience with the current policy and then updates it.
Like Aync, VER does not have synchronization points &ndash; it computes next actions, steps environments, and updates the policy as soon as possible.
The inspiration for VER comes from two key observations:</p>

<ul>
<li>Aync mitigates the straggler effect by implicitly collecting a variable amount of experience from each environment &ndash; more from fast-to-simulate environments and less from slow ones.</li>
<li>Both Sync and Aync
use a fixed rollout length, $T$ steps of experience.
Our key insight is that while a fixed rollout length may simplify an implementation, it is \emph{not} a requirement for RL.</li>
</ul>

<p>These two key observations naturally  lead us to
<em>variable experience rollout</em> (VER), <em>i.e.</em> collecting rollouts with a variable number of steps.
VER adjusts the rollout length for each environment based on its simulation speed.
It explicitly collects more experience from fast-to-simulate environments and less from slow ones.
The result is an RL system that
overcomes the straggler effect <em>and</em> maintains sample-efficiency
by learning from on-policy data.</p>

<p>VER focuses on efficiently utilizing a single GPU.
To enable efficient scaling to multiple GPUs, we combine VER with the decentralized distributed method proposed in <a href="/publication/ddppo-2019">DD-PPO</a>.</p>

<h1 id="benchmarking-navigation">Benchmarking: Navigation</h1>

<p><img src="/files/ver-post/ver-nav-benchmarking.jpg" alt="VER compared to DD-PPO on navigation tasks" /></p>

<p>First, we evaluate VER on well-established embodied navigation tasks using <a href="/publication/hab2">Habitat 1.0</a>
on 8 GPUs.
VER trains PointGoal navigation 60% faster than
DD-PPO, the current state-of-the-art for distributed on-policy RL, with the same sample efficiency.
For ObjectGoal navigation, an active area of research, VER uses 340% less compute than DD-PPO with (slightly) better sample efficiency.</p>

<h1 id="benchmarking-rearrangement">Benchmarking: Rearrangement</h1>

<p><img src="/files/ver-post/ver-rearrange-benchmarking.jpg" alt="VER compared to DD-PPO and SampleFactory on rearrangement tasks" /></p>

<p>Next, we evaluate VER on
the recently introduced (and significantly more challenging) GeometricGoal rearrangement tasks in Habitat 2.0.
In GeoRearrange, a virtual robot is spawned in a new environment and asked to rearrange a set of objects from their initial to desired coordinates.
These environments have highly variable simulation time (physics simulation time increases if the robot bumps into something) and require GPU-acceleration (for photo-realistic rendering), limiting the number of environments that can be run in parallel.</p>

<p>On 1 GPU, VER is 150% faster (2.5x speedup) than DD-PPO
with the same sample efficiency.
VER is as fast as SampleFactory, the state-of-the-art Async,
with the same sample efficiency.
VER is as fast as Aysnc in pure throughput; this is a surprisingly strong result. Async never stops collecting experience and should, in theory, be a strict upper bound on performance.
VER is able to match Async for environments that heavily utilize the GPU for rendering, like Habitat.
In Async, learning, inference, and rendering contend the GPU which reduces throughput.
In VER, inference and rendering contend for the GPU while learning does not.</p>

<p>On 8 GPUs, VER achieves better scaling than DD-PPO, achieving a 6.7x speed-up (vs.~6x for DD-PPO) due to lower variance in experience collection time between GPU-workers.
Due to this efficient multi-GPU scaling, VER is 70% faster (1.7x speedup) than SampleFactory on 8 GPUs and has better sample efficiency as it learns from on-policy data.</p>

<h1 id="embodied-rearrangement-emergence-of-navigation">Embodied Rearrangement: Emergence of Navigation</h1>

<p>Finally, we leverage these SysML contributions to study open research questions posed in prior work.
Specifically, we train RL policies for mobile manipulation skills (Navigate, Pick, Place, <em>etc</em>) and chain them via a task planner.
Prior work <a href="/publication/hab2">(Szot et al, 2021)</a> called this approach TP-SRL and identified a critical &ldquo;handoff problem&rdquo; &ndash; downstream skills are set up for failure by small errors made by upstream skills (<em>e.g.</em>
the Pick skill failing because the navigation skill stopped the robot a bit too far from the object
).</p>

<p>We demonstrate a number of surprising findings when TP-SRL is scaled via VER.
Most importantly, we find the <em>emergence of navigation</em> when skills that do not ostensibly require navigation (Pick) are trained with navigation actions enabled.
In principle, Pick and Place policies do not <em>need</em> to navigate during training since the objects are always in arm&rsquo;s reach, but in practice they learn to navigate to recover from their mistakes and this results in strong out-of-distribution test-time generalization.
Specifically, TP-SRL <em>without a navigation skill</em> achieves 50% success on NavPick and 20% success on a NavPickNavPlace task simply because the Pick and Pace skills have learned to navigate (sometimes across the room!).
TP-SRL with a Navigate skill performs even stronger: 90% on NavPickNavPlace and 32% on 5 successive NavPickNavPlaces (called Tidy House), which are +32% and +30% absolute improvements over Szot et al 2021, respectively.
Prepare Groceries and Set Table, which both require interaction with articulated receptacles (fridge, drawer), remain as open problems (5% and 0% Success, respectively) and are the next frontiers.</p>

<h2 id="example-of-tp-srl-without-a-navigation-skill">Example of TP-SRL without a navigation skill</h2>

<video height="100%" width="100%" controls autoplay>
<source src="/files/ver-post/1-tp-srl-no-nav.mp4" type="video/mp4">
</video>

<h2 id="examples-of-pick-place-during-training">Examples of Pick/Place during training</h2>

<h3 id="pick">Pick</h3>

<video height="100%" width="100%" controls autoplay>
<source src="/files/ver-post/3-pick-from-table.mp4" type="video/mp4">
</video>

<video height="100%" width="100%" controls autoplay>
<source src="/files/ver-post/3-pick-from-fridge.mp4" type="video/mp4">
</video>

<h3 id="place">Place</h3>

<video height="100%" width="100%" controls autoplay>
<source src="/files/ver-post/4-place-on-table.mp4" type="video/mp4">
</video>

<video height="100%" width="100%" controls autoplay>
<source src="/files/ver-post/4-place-into-drawer.mp4" type="video/mp4">
</video>

<p>Notice  how much closer the policies are initialized during training than in the video of TP-SRL without a navigation skill.</p>

<h2 id="tp-srl-with-a-navigation-skill">TP-SRL with a Navigation Skill</h2>

<video height="100%" width="100%" controls autoplay>
<source src="/files/ver-post/2-tp-srl.mp4" type="video/mp4">
</video>
</div>

    





  </div>
</div>



<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2020 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

