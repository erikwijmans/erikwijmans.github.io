<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Erik Wijmans on Erik Wijmans</title>
    <link>https://wijmans.xyz/</link>
    <description>Recent content in Erik Wijmans on Erik Wijmans</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2020</copyright>
    <lastBuildDate>Mon, 19 Feb 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>VER: Scaling On-Policy RL Leads to Emergence of Navigation in Embodied Rearrangement</title>
      <link>https://wijmans.xyz/publication/ver/</link>
      <pubDate>Sun, 28 Aug 2022 14:53:59 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/ver/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;How can we combine the benefit of synchronous (high sample efficiency) and asynchronous (high throughput) systems for batched on-policy reinforcement learning?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wijmans.xyz/files/ver-post/ver-teaser-v4.jpg&#34; alt=&#34;VER System Overview&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Systems for batched on-policy reinforcement learning
collect experience from many (N) environments simultaneously using the policy and update it with this cumulative experience.
They are broadly divided into two classes: synchronous (Sync) and asynchronous (Aync).
Sync contains two synchronization points: first the policy is executed for the entire batch $(o_t \rightarrow a_t)^B_{b=1}$ (A), then actions are executed in &lt;em&gt;all&lt;/em&gt; environments,
$(s_t, a_t \rightarrow s_{t+1}, o_{t+1})^B_{b=1}$ (B), until $T$ steps have been collected from all $N$ environments.
This $(T, N)$-shaped batch of experience is used to update the policy &amp;copy;.
Synchronization reduces throughput as
the system spends significant (sometimes the most) time waiting for the slowest environment to finish.
This known as is the straggler effect.&lt;/p&gt;

&lt;p&gt;Aync removes these synchronization points, thereby mitigating the straggler effect and improving throughput.
Actions are taken as soon as they are computed, $a_t \rightarrow o_{t+1}$ (D), the next action is computed as soon as the observation is ready, $o_t \rightarrow a_t$, and the policy is updated as soon as enough experience is collected (E).
However, Aync systems are not able to ensure that all experience has been collected by only the current policy and thus must consume &lt;em&gt;near&lt;/em&gt;-policy data.
This reduces sample efficiency.
Thus, status quo leaves us with
 an unpleasant tradeoff &amp;ndash; high sample-efficiency with low throughput or high throughput with low sample-efficiency.&lt;/p&gt;

&lt;p&gt;In this work, we propose Variable Experience Rollout (VER).
VER
combines the strengths of and blurs the line between
Sync and Aync.
Like Sync, VER collects experience with the current policy and then updates it.
Like Aync, VER does not have synchronization points &amp;ndash; it computes next actions, steps environments, and updates the policy as soon as possible.
The inspiration for VER comes from two key observations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Aync mitigates the straggler effect by implicitly collecting a variable amount of experience from each environment &amp;ndash; more from fast-to-simulate environments and less from slow ones.&lt;/li&gt;
&lt;li&gt;Both Sync and Aync
use a fixed rollout length, $T$ steps of experience.
Our key insight is that while a fixed rollout length may simplify an implementation, it is \emph{not} a requirement for RL.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These two key observations naturally  lead us to
&lt;em&gt;variable experience rollout&lt;/em&gt; (VER), &lt;em&gt;i.e.&lt;/em&gt; collecting rollouts with a variable number of steps.
VER adjusts the rollout length for each environment based on its simulation speed.
It explicitly collects more experience from fast-to-simulate environments and less from slow ones.
The result is an RL system that
overcomes the straggler effect &lt;em&gt;and&lt;/em&gt; maintains sample-efficiency
by learning from on-policy data.&lt;/p&gt;

&lt;p&gt;VER focuses on efficiently utilizing a single GPU.
To enable efficient scaling to multiple GPUs, we combine VER with the decentralized distributed method proposed in &lt;a href=&#34;https://wijmans.xyz/publication/ddppo-2019&#34;&gt;DD-PPO&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;benchmarking-navigation&#34;&gt;Benchmarking: Navigation&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://wijmans.xyz/files/ver-post/ver-nav-benchmarking.jpg&#34; alt=&#34;VER compared to DD-PPO on navigation tasks&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First, we evaluate VER on well-established embodied navigation tasks using &lt;a href=&#34;https://wijmans.xyz/publication/hab2&#34;&gt;Habitat 1.0&lt;/a&gt;
on 8 GPUs.
VER trains PointGoal navigation 60% faster than
DD-PPO, the current state-of-the-art for distributed on-policy RL, with the same sample efficiency.
For ObjectGoal navigation, an active area of research, VER uses 340% less compute than DD-PPO with (slightly) better sample efficiency.&lt;/p&gt;

&lt;h1 id=&#34;benchmarking-rearrangement&#34;&gt;Benchmarking: Rearrangement&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://wijmans.xyz/files/ver-post/ver-rearrange-benchmarking.jpg&#34; alt=&#34;VER compared to DD-PPO and SampleFactory on rearrangement tasks&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Next, we evaluate VER on
the recently introduced (and significantly more challenging) GeometricGoal rearrangement tasks in Habitat 2.0.
In GeoRearrange, a virtual robot is spawned in a new environment and asked to rearrange a set of objects from their initial to desired coordinates.
These environments have highly variable simulation time (physics simulation time increases if the robot bumps into something) and require GPU-acceleration (for photo-realistic rendering), limiting the number of environments that can be run in parallel.&lt;/p&gt;

&lt;p&gt;On 1 GPU, VER is 150% faster (2.5x speedup) than DD-PPO
with the same sample efficiency.
VER is as fast as SampleFactory, the state-of-the-art Async,
with the same sample efficiency.
VER is as fast as Aysnc in pure throughput; this is a surprisingly strong result. Async never stops collecting experience and should, in theory, be a strict upper bound on performance.
VER is able to match Async for environments that heavily utilize the GPU for rendering, like Habitat.
In Async, learning, inference, and rendering contend the GPU which reduces throughput.
In VER, inference and rendering contend for the GPU while learning does not.&lt;/p&gt;

&lt;p&gt;On 8 GPUs, VER achieves better scaling than DD-PPO, achieving a 6.7x speed-up (vs.~6x for DD-PPO) due to lower variance in experience collection time between GPU-workers.
Due to this efficient multi-GPU scaling, VER is 70% faster (1.7x speedup) than SampleFactory on 8 GPUs and has better sample efficiency as it learns from on-policy data.&lt;/p&gt;

&lt;h1 id=&#34;embodied-rearrangement-emergence-of-navigation&#34;&gt;Embodied Rearrangement: Emergence of Navigation&lt;/h1&gt;

&lt;p&gt;Finally, we leverage these SysML contributions to study open research questions posed in prior work.
Specifically, we train RL policies for mobile manipulation skills (Navigate, Pick, Place, &lt;em&gt;etc&lt;/em&gt;) and chain them via a task planner.
Prior work &lt;a href=&#34;https://wijmans.xyz/publication/hab2&#34;&gt;(Szot et al, 2021)&lt;/a&gt; called this approach TP-SRL and identified a critical &amp;ldquo;handoff problem&amp;rdquo; &amp;ndash; downstream skills are set up for failure by small errors made by upstream skills (&lt;em&gt;e.g.&lt;/em&gt;
the Pick skill failing because the navigation skill stopped the robot a bit too far from the object
).&lt;/p&gt;

&lt;p&gt;We demonstrate a number of surprising findings when TP-SRL is scaled via VER.
Most importantly, we find the &lt;em&gt;emergence of navigation&lt;/em&gt; when skills that do not ostensibly require navigation (Pick) are trained with navigation actions enabled.
In principle, Pick and Place policies do not &lt;em&gt;need&lt;/em&gt; to navigate during training since the objects are always in arm&amp;rsquo;s reach, but in practice they learn to navigate to recover from their mistakes and this results in strong out-of-distribution test-time generalization.
Specifically, TP-SRL &lt;em&gt;without a navigation skill&lt;/em&gt; achieves 50% success on NavPick and 20% success on a NavPickNavPlace task simply because the Pick and Pace skills have learned to navigate (sometimes across the room!).
TP-SRL with a Navigate skill performs even stronger: 90% on NavPickNavPlace and 32% on 5 successive NavPickNavPlaces (called Tidy House), which are +32% and +30% absolute improvements over Szot et al 2021, respectively.
Prepare Groceries and Set Table, which both require interaction with articulated receptacles (fridge, drawer), remain as open problems (5% and 0% Success, respectively) and are the next frontiers.&lt;/p&gt;

&lt;h2 id=&#34;example-of-tp-srl-without-a-navigation-skill&#34;&gt;Example of TP-SRL without a navigation skill&lt;/h2&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/1-tp-srl-no-nav.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;h2 id=&#34;examples-of-pick-place-during-training&#34;&gt;Examples of Pick/Place during training&lt;/h2&gt;

&lt;h3 id=&#34;pick&#34;&gt;Pick&lt;/h3&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/3-pick-from-table.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/3-pick-from-fridge.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;h3 id=&#34;place&#34;&gt;Place&lt;/h3&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/4-place-on-table.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/4-place-into-drawer.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;Notice  how much closer the policies are initialized during training than in the video of TP-SRL without a navigation skill.&lt;/p&gt;

&lt;h2 id=&#34;tp-srl-with-a-navigation-skill&#34;&gt;TP-SRL with a Navigation Skill&lt;/h2&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/2-tp-srl.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Is Mapping Necessary for Realistic PointGoal Navigation?</title>
      <link>https://wijmans.xyz/publication/mapping-for-pn/</link>
      <pubDate>Sat, 27 Aug 2022 15:05:01 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/mapping-for-pn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI</title>
      <link>https://wijmans.xyz/publication/hm3d/</link>
      <pubDate>Tue, 05 Oct 2021 18:28:47 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/hm3d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Megaverse Hacker News</title>
      <link>https://wijmans.xyz/press/megaverse-hacker-news/</link>
      <pubDate>Sat, 31 Jul 2021 14:36:42 -0400</pubDate>
      
      <guid>https://wijmans.xyz/press/megaverse-hacker-news/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hab2 Fb Ai</title>
      <link>https://wijmans.xyz/press/hab2-fb-ai/</link>
      <pubDate>Wed, 30 Jun 2021 11:46:43 -0400</pubDate>
      
      <guid>https://wijmans.xyz/press/hab2-fb-ai/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hab2 Cnn</title>
      <link>https://wijmans.xyz/press/hab2-cnn/</link>
      <pubDate>Wed, 30 Jun 2021 11:44:01 -0400</pubDate>
      
      <guid>https://wijmans.xyz/press/hab2-cnn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Habitat 2.0: Training Home Assistants to Rearrange their Habitat</title>
      <link>https://wijmans.xyz/publication/hab2/</link>
      <pubDate>Wed, 30 Jun 2021 11:13:40 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/hab2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Auxiliary Tasks and Exploration Enable ObjectNav</title>
      <link>https://wijmans.xyz/publication/aux-tasks-obj-nav/</link>
      <pubDate>Wed, 30 Jun 2021 11:13:34 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/aux-tasks-obj-nav/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Megaverse: Simulating Embodied Agents at One Million Experiences per Second</title>
      <link>https://wijmans.xyz/publication/megaverse/</link>
      <pubDate>Wed, 30 Jun 2021 11:13:16 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/megaverse/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2021 Georgia Tech College of Compute Outstanding Graduate Research Assistant Award</title>
      <link>https://wijmans.xyz/award/coc-gra-2021/</link>
      <pubDate>Wed, 31 Mar 2021 23:18:56 -0400</pubDate>
      
      <guid>https://wijmans.xyz/award/coc-gra-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Large Batch Simulation for Deep Reinforcement Learning</title>
      <link>https://wijmans.xyz/publication/bps-iclr-2021/</link>
      <pubDate>Sun, 14 Mar 2021 22:14:32 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/bps-iclr-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Train PointGoal Navigation Agents on a (Sample and Compute) Budget</title>
      <link>https://wijmans.xyz/publication/pointnav-in-a-day/</link>
      <pubDate>Mon, 14 Dec 2020 11:14:39 -0500</pubDate>
      
      <guid>https://wijmans.xyz/publication/pointnav-in-a-day/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2020-21 Scholar Award fellowship from the Achievement Rewards for College Scientists (ARCS) Foundation</title>
      <link>https://wijmans.xyz/award/arcs-20/</link>
      <pubDate>Sun, 20 Sep 2020 11:38:56 -0400</pubDate>
      
      <guid>https://wijmans.xyz/award/arcs-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cnn Habitat 08 2020</title>
      <link>https://wijmans.xyz/press/cnn-habitat-08-2020/</link>
      <pubDate>Fri, 18 Sep 2020 11:06:42 -0400</pubDate>
      
      <guid>https://wijmans.xyz/press/cnn-habitat-08-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Auxiliary Tasks Speed Up Learning PointGoal Navigation</title>
      <link>https://wijmans.xyz/publication/habitat-memory-aux/</link>
      <pubDate>Thu, 09 Jul 2020 20:50:26 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/habitat-memory-aux/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
