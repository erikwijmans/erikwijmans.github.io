<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Erik Wijmans on Erik Wijmans</title>
    <link>https://wijmans.xyz/</link>
    <description>Recent content in Erik Wijmans on Erik Wijmans</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2020</copyright>
    <lastBuildDate>Mon, 19 Feb 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Cut Your Losses in Large-Vocabulary Language Models</title>
      <link>https://wijmans.xyz/publication/cce/</link>
      <pubDate>Sun, 17 Nov 2024 19:21:40 -0500</pubDate>
      
      <guid>https://wijmans.xyz/publication/cce/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Does Spatial Cognition Emerge in Frontier Models?</title>
      <link>https://wijmans.xyz/publication/space/</link>
      <pubDate>Wed, 09 Oct 2024 19:21:31 -0500</pubDate>
      
      <guid>https://wijmans.xyz/publication/space/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Outstanding Paper Award, International Conference on Learning Representations (ICLR), 2023</title>
      <link>https://wijmans.xyz/award/eom-best-paper/</link>
      <pubDate>Tue, 21 Mar 2023 13:18:39 -0400</pubDate>
      
      <guid>https://wijmans.xyz/award/eom-best-paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Eom New Scientist</title>
      <link>https://wijmans.xyz/press/eom-new-scientist/</link>
      <pubDate>Tue, 14 Mar 2023 17:11:30 -0400</pubDate>
      
      <guid>https://wijmans.xyz/press/eom-new-scientist/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sigma Xi Best PhD Thesis Award, 2023</title>
      <link>https://wijmans.xyz/award/sigma-xi-thesis-2023/</link>
      <pubDate>Tue, 14 Mar 2023 16:59:52 -0400</pubDate>
      
      <guid>https://wijmans.xyz/award/sigma-xi-thesis-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2023 Georgia Tech College of Computing Best Doctoral Dissertation Award</title>
      <link>https://wijmans.xyz/award/coc-thesis-2023/</link>
      <pubDate>Wed, 01 Mar 2023 11:38:40 -0400</pubDate>
      
      <guid>https://wijmans.xyz/award/coc-thesis-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emergence of Maps in the Memories of Blind Navigation Agents</title>
      <link>https://wijmans.xyz/publication/eom/</link>
      <pubDate>Sun, 22 Jan 2023 12:10:17 -0500</pubDate>
      
      <guid>https://wijmans.xyz/publication/eom/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Decades of research into intelligent animal navigation posits that organisms build and maintain inter-
nal spatial representations (or maps) of their environment, that enables the organism to determine
and follow task-appropriate paths.
Hamsters, wolves, chimpanzees, and bats leverage prior exploration to determine and follow short-
cuts they may never have taken before.  Even blind mole rats and animals rendered situationally-blind in dark environments demonstrate shortcut behaviors. Ants forage for food along meandering paths but take near-optimal
return trips, though there is some controversy about whether insects like
ants and bees are capable of forming maps.&lt;/p&gt;

&lt;p&gt;Analogously, mapping and localization techniques have long played a central role in enabling non-
biological navigation agents (or robots) to exhibit intelligent behavior.
More recently, the machine learning community has produced a surprising phenomenon – neural-network models for navigation that curiously
do not contain any explicit mapping modules but still achieve remarkably high performance For instance, Wijmans et al. (2020) showed that
a simple ‘pixels-to-actions’ architecture (using a CNN and RNN) can navigate to a given point in
a novel environment with near-perfect accuracy; Partsey et al. (2022) further generalized this result
to more realistic sensors and actuators. Reed et al. (2022) showed a similar general purpose architecture (a transformer) can perform a wide variety of embodied tasks, including navigation. The
mechanisms explaining this ability remain unknown. Understanding them is both of scientific and
practical importance due to safety considerations involved with deploying such systems.&lt;/p&gt;

&lt;p&gt;In this work, we investigate the following question – is mapping an emergent phenomenon? Specifically, do artificial intelligence (AI) agents learn to build internal spatial representations (or ‘mental’
maps) of their environment as a natural consequence of learning to navigate?&lt;/p&gt;

&lt;h1 id=&#34;task&#34;&gt;Task&lt;/h1&gt;

&lt;p&gt;The specific task we study is PointGoal navigation, where an AI agent is
introduced into a new (unexplored) environment and tasked with navigating to a relative location –
&amp;lsquo;&lt;em&gt;go 5m north, 2m west relative to start&lt;/em&gt;&amp;rsquo;. This is analogous to the direction and distance of foraging
locations communicated by the waggle dance of honey bees.&lt;/p&gt;

&lt;h1 id=&#34;agent-input-and-design&#34;&gt;Agent input and design&lt;/h1&gt;

&lt;p&gt;Unlike animal navigation studies, experiments with AI agents allow us to precisely isolate map-
ping from alternative mechanisms proposed for animal navigation – the use of visual land-
marks orientation by the arrangement of stars, gradients of
olfaction or other senses. We achieve this isolation by judiciously designing
the agent’s perceptual system and the learning paradigm such that these alternative mechanisms are
rendered implausible. Our agents are effectively ‘blind’; they possess a minimal perceptual system
capable of sensing only egomotion, i.e. change in the agent’s location and orientation as the it moves
– no vision, no audio, no olfactory, no haptic, no magnetic, or any other sensing of any kind. This
perceptual system is deliberately impoverished to isolate the contribution of memory, and is inspired
by blind mole rats, who perform localization via path integration and use the Earth’s magnetic field
as a compass. Further still, our agents are composed of navigation-agnostic,
generic, and ubiquitous architectural components (fully-connected layers and LSTM-based recurrent neural networks), and our experimental setup provides no inductive bias towards mapping – no
map-like or spatial structural components in the agent, no mapping supervision, no auxiliary tasks,
nothing other than a reward for making progress towards a goal.&lt;/p&gt;

&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;

&lt;p&gt;Surprisingly, even under these deliberately harsh conditions, we find the emergence of map-like
spatial representations in the agent’s non-spatial unstructured memory, enabling it to not only successfully navigate to the goal but also exhibit intelligent behavior (like taking shortcuts, following
walls, detecting collisions) similar to aforementioned animal studies, and predict free-space in the
environment. Essentially, we demonstrate an ‘existence proof’ or an ontogenetic developmental ac-
count for the emergence of mapping without any previous predisposition. Our results also explain
the aforementioned surprising finding in recent literature – that ostensibly map-free neural-network
achieve strong autonomous navigation performance – by demonstrating that these ‘map-free’ systems in fact learn to construct and maintain map-like representations of their environment.&lt;/p&gt;

&lt;p&gt;Concretely, we ask and answer following questions:&lt;/p&gt;

&lt;h2 id=&#34;is-it-possible-to-effectively-navigate-with-just-egomotion-sensing&#34;&gt;Is it possible to effectively navigate with just egomotion sensing?&lt;/h2&gt;

&lt;p&gt;Yes. We find that our ‘blind’
agents are highly effective in navigating new environments – reaching the goal with 95.1%$\pm$1.3%
success rate. And they traverse moderately efficient (though far from optimal) paths, reaching
62.9%$\pm$1.6% of optimal path efficiency. We stress that these are novel testing environments, the
agent has not memorized paths within a training environment but has learned efficient navigation
strategies that generalize to novel environments, such as emergent wall-following behavior.&lt;/p&gt;

&lt;h2 id=&#34;what-mechanism-explains-this-strong-performance-by-blind-agents&#34;&gt;What mechanism explains this strong performance by ‘blind’ agents?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://wijmans.xyz/files/eom-post/perf-vs-mem-len-post-hoc.jpg&#34; alt=&#34;Blind Agent Performance&#34; width=&#34;50%&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Memory.
We find that
memoryless agents completely fail at this task, achieving nearly 0% success. More importantly,
we find that agents with memory utilize information stored over a long temporal and spatial horizon and that collision-detection neurons emerge within this memory. Navigation performance as
a function of the number of past actions/observations encoded in the agent’s memory does not saturate till one thousand steps (corresponding to the agent traversing 89.1$\pm$0.66 meters), suggest-
ing that the agent ‘remembers’ a long history of the episode.&lt;/p&gt;

&lt;p&gt;The video bellow shows a video of a blind agent navigating (right) and the t-SNE projection of the portion of it&amp;rsquo;s hidden state responsible for
detecting collision. The black dot is where the agent&amp;rsquo;s hidden state is for the current state. Notice that the hidden state stays within the same
cluster throughout a series of actions.&lt;/p&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/eom-post/collision-neuron-vis.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;h2 id=&#34;what-information-does-the-memory-encode-about-the-environment&#34;&gt;What information does the memory encode about the environment?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://wijmans.xyz/files/eom-post/occupancy-map-figure-good-only.jpg&#34; alt=&#34;Decoded Occupancy Maps&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Implicit maps. We perform
an AI rendition of Menzel (1973)’s experiments, where a chimpanzee is carried by a human and
shown the location of food hidden in the environment. When the animal is set free to collect the
food, it does not retrace the demonstrator’s steps but takes shortcuts to collect the food faster.
Analogously, we train a blind agent to navigate from a source location (S) to a target location
(T). After it has finished navigating, we transplant its constructed episodic memory into a second
‘probe’-agent (which is also blind). We find that this implanted-memory probe-agent performs
dramatically better in navigating from S to T (and T to S) than it would without the memory
transplant. Similar to the chimpanzee, the probe agent takes shortcuts, typically cutting out
backtracks or excursions that the memory-creator had undertaken as it tried to work its way
around the obstacles. These experiments provide compelling evidence that blind agents learn to
build and use implicit map-like representations of their environment solely through learning to
navigate. Intriguingly further still, we find that surprisingly detailed metric occupancy maps of
the environment (indicating free-space) can be explicitly decoded from the agent’s memory.&lt;/p&gt;

&lt;p&gt;This videos show a blind agent navigating from source to target and then a probe navigating from target to source.
Notice the increase efficiency and reduction in collisions of the probe during the return trip.&lt;/p&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/eom-post/probe-example.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;h2 id=&#34;are-maps-task-dependent&#34;&gt;Are maps task-dependent?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://wijmans.xyz/files/eom-post/excursion-forgetting.jpg&#34; alt=&#34;Excursion Forgetting&#34; width=&#34;75%&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Yes. We find that the emergent maps are a function of the navigation
goal. Agents &amp;lsquo;forget&amp;rsquo; excursions and detours, &lt;em&gt;i.e.&lt;/em&gt; their episodic memory only preserves the
features of the environment relevant to navigating to their goal. This, in part, explains why
transplanting episodic memory from one agent to another leads it to take shortcuts – because the
excursion and detours are simply forgotten.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Overall, our experiments and analyses demonstrate that ‘blind’ agents solve PointGoalNav by
combining information over long time horizons to build detailed maps of their environment, solely
through the learning signals imposed by goal-driven navigation. In biological systems, convergent
evolution of analogous structures that cannot be attributed to a common ancestor (&lt;em&gt;e.g.&lt;/em&gt; eyes in
vertebrates and jellyfish) is often an indicator that the structure is a natural
response to the ecological niche and selection pressures. Analogously, our results suggest that
mapping may be a natural solution to the problem of navigation by intelligent embodied agents,
whether they be biological or artificial. We now describe our findings for each question in detail.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav</title>
      <link>https://wijmans.xyz/publication/pirl-nav/</link>
      <pubDate>Sun, 22 Jan 2023 12:09:57 -0500</pubDate>
      
      <guid>https://wijmans.xyz/publication/pirl-nav/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2021-22 Scholar Award fellowship from the Achievement Rewards for College Scientists (ARCS) Foundation</title>
      <link>https://wijmans.xyz/award/arcs-2021/</link>
      <pubDate>Wed, 16 Nov 2022 17:36:25 -0500</pubDate>
      
      <guid>https://wijmans.xyz/award/arcs-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Replica Neurohive</title>
      <link>https://wijmans.xyz/press/replica-neurohive/</link>
      <pubDate>Wed, 16 Nov 2022 17:30:19 -0500</pubDate>
      
      <guid>https://wijmans.xyz/press/replica-neurohive/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hab2 Vb</title>
      <link>https://wijmans.xyz/press/hab2-vb/</link>
      <pubDate>Wed, 16 Nov 2022 17:28:30 -0500</pubDate>
      
      <guid>https://wijmans.xyz/press/hab2-vb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hab2 Tech Crunch</title>
      <link>https://wijmans.xyz/press/hab2-tech-crunch/</link>
      <pubDate>Wed, 16 Nov 2022 17:26:36 -0500</pubDate>
      
      <guid>https://wijmans.xyz/press/hab2-tech-crunch/</guid>
      <description></description>
    </item>
    
    <item>
      <title>VER: Scaling On-Policy RL Leads to Emergence of Navigation in Embodied Rearrangement</title>
      <link>https://wijmans.xyz/publication/ver/</link>
      <pubDate>Sun, 28 Aug 2022 14:53:59 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/ver/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;How can we combine the benefit of synchronous (high sample efficiency) and asynchronous (high throughput) systems for batched on-policy reinforcement learning?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wijmans.xyz/files/ver-post/ver-teaser-v4.jpg&#34; alt=&#34;VER System Overview&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Systems for batched on-policy reinforcement learning
collect experience from many (N) environments simultaneously using the policy and update it with this cumulative experience.
They are broadly divided into two classes: synchronous (Sync) and asynchronous (Aync).
Sync contains two synchronization points: first the policy is executed for the entire batch $(o_t \rightarrow a_t)^B_{b=1}$ (A), then actions are executed in &lt;em&gt;all&lt;/em&gt; environments,
$(s_t, a_t \rightarrow s_{t+1}, o_{t+1})^B_{b=1}$ (B), until $T$ steps have been collected from all $N$ environments.
This $(T, N)$-shaped batch of experience is used to update the policy &amp;copy;.
Synchronization reduces throughput as
the system spends significant (sometimes the most) time waiting for the slowest environment to finish.
This known as is the straggler effect.&lt;/p&gt;

&lt;p&gt;Aync removes these synchronization points, thereby mitigating the straggler effect and improving throughput.
Actions are taken as soon as they are computed, $a_t \rightarrow o_{t+1}$ (D), the next action is computed as soon as the observation is ready, $o_t \rightarrow a_t$, and the policy is updated as soon as enough experience is collected (E).
However, Aync systems are not able to ensure that all experience has been collected by only the current policy and thus must consume &lt;em&gt;near&lt;/em&gt;-policy data.
This reduces sample efficiency.
Thus, status quo leaves us with
 an unpleasant tradeoff &amp;ndash; high sample-efficiency with low throughput or high throughput with low sample-efficiency.&lt;/p&gt;

&lt;p&gt;In this work, we propose Variable Experience Rollout (VER).
VER
combines the strengths of and blurs the line between
Sync and Aync.
Like Sync, VER collects experience with the current policy and then updates it.
Like Aync, VER does not have synchronization points &amp;ndash; it computes next actions, steps environments, and updates the policy as soon as possible.
The inspiration for VER comes from two key observations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Aync mitigates the straggler effect by implicitly collecting a variable amount of experience from each environment &amp;ndash; more from fast-to-simulate environments and less from slow ones.&lt;/li&gt;
&lt;li&gt;Both Sync and Aync
use a fixed rollout length, $T$ steps of experience.
Our key insight is that while a fixed rollout length may simplify an implementation, it is &lt;em&gt;not&lt;/em&gt; a requirement for RL.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These two key observations naturally  lead us to
&lt;em&gt;variable experience rollout&lt;/em&gt; (VER), &lt;em&gt;i.e.&lt;/em&gt; collecting rollouts with a variable number of steps.
VER adjusts the rollout length for each environment based on its simulation speed.
It explicitly collects more experience from fast-to-simulate environments and less from slow ones.
The result is an RL system that
overcomes the straggler effect &lt;em&gt;and&lt;/em&gt; maintains sample-efficiency
by learning from on-policy data.&lt;/p&gt;

&lt;p&gt;VER focuses on efficiently utilizing a single GPU.
To enable efficient scaling to multiple GPUs, we combine VER with the decentralized distributed method proposed in &lt;a href=&#34;https://wijmans.xyz/publication/ddppo-2019&#34;&gt;DD-PPO&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;benchmarking-navigation&#34;&gt;Benchmarking: Navigation&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://wijmans.xyz/files/ver-post/ver-nav-benchmarking.jpg&#34; alt=&#34;VER compared to DD-PPO on navigation tasks&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First, we evaluate VER on well-established embodied navigation tasks using &lt;a href=&#34;https://wijmans.xyz/publication/ai-habitat&#34;&gt;Habitat 1.0&lt;/a&gt;
on 8 GPUs.
VER trains PointGoal navigation 60% faster than
DD-PPO, the current state-of-the-art for distributed on-policy RL, with the same sample efficiency.
For ObjectGoal navigation, an active area of research, VER uses 340% less compute than DD-PPO with (slightly) better sample efficiency.&lt;/p&gt;

&lt;h1 id=&#34;benchmarking-rearrangement&#34;&gt;Benchmarking: Rearrangement&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://wijmans.xyz/files/ver-post/ver-rearrange-benchmarking.jpg&#34; alt=&#34;VER compared to DD-PPO and SampleFactory on rearrangement tasks&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Next, we evaluate VER on
the recently introduced (and significantly more challenging) GeometricGoal rearrangement tasks in Habitat 2.0.
In GeoRearrange, a virtual robot is spawned in a new environment and asked to rearrange a set of objects from their initial to desired coordinates.
These environments have highly variable simulation time (physics simulation time increases if the robot bumps into something) and require GPU-acceleration (for photo-realistic rendering), limiting the number of environments that can be run in parallel.&lt;/p&gt;

&lt;p&gt;On 1 GPU, VER is 150% faster (2.5x speedup) than DD-PPO
with the same sample efficiency.
VER is as fast as SampleFactory, the state-of-the-art Async,
with the same sample efficiency.
VER is as fast as Aysnc in pure throughput; this is a surprisingly strong result. Async never stops collecting experience and should, in theory, be a strict upper bound on performance.
VER is able to match Async for environments that heavily utilize the GPU for rendering, like Habitat.
In Async, learning, inference, and rendering contend the GPU which reduces throughput.
In VER, inference and rendering contend for the GPU while learning does not.&lt;/p&gt;

&lt;p&gt;On 8 GPUs, VER achieves better scaling than DD-PPO, achieving a 6.7x speed-up (vs.~6x for DD-PPO) due to lower variance in experience collection time between GPU-workers.
Due to this efficient multi-GPU scaling, VER is 70% faster (1.7x speedup) than SampleFactory on 8 GPUs and has better sample efficiency as it learns from on-policy data.&lt;/p&gt;

&lt;h1 id=&#34;embodied-rearrangement-emergence-of-navigation&#34;&gt;Embodied Rearrangement: Emergence of Navigation&lt;/h1&gt;

&lt;p&gt;Finally, we leverage these SysML contributions to study open research questions posed in prior work.
Specifically, we train RL policies for mobile manipulation skills (Navigate, Pick, Place, &lt;em&gt;etc&lt;/em&gt;) and chain them via a task planner.
Prior work &lt;a href=&#34;https://wijmans.xyz/publication/hab2&#34;&gt;(Szot et al, 2021)&lt;/a&gt; called this approach TP-SRL and identified a critical &amp;ldquo;handoff problem&amp;rdquo; &amp;ndash; downstream skills are set up for failure by small errors made by upstream skills (&lt;em&gt;e.g.&lt;/em&gt;
the Pick skill failing because the navigation skill stopped the robot a bit too far from the object
).&lt;/p&gt;

&lt;p&gt;We demonstrate a number of surprising findings when TP-SRL is scaled via VER.
Most importantly, we find the &lt;em&gt;emergence of navigation&lt;/em&gt; when skills that do not ostensibly require navigation (Pick) are trained with navigation actions enabled.
In principle, Pick and Place policies do not &lt;em&gt;need&lt;/em&gt; to navigate during training since the objects are always in arm&amp;rsquo;s reach, but in practice they learn to navigate to recover from their mistakes and this results in strong out-of-distribution test-time generalization.
Specifically, TP-SRL &lt;em&gt;without a navigation skill&lt;/em&gt; achieves 50% success on NavPick and 20% success on a NavPickNavPlace task simply because the Pick and Pace skills have learned to navigate (sometimes across the room!).
TP-SRL with a Navigate skill performs even stronger: 90% on NavPickNavPlace and 32% on 5 successive NavPickNavPlaces (called Tidy House), which are +32% and +30% absolute improvements over Szot et al 2021, respectively.
Prepare Groceries and Set Table, which both require interaction with articulated receptacles (fridge, drawer), remain as open problems (5% and 0% Success, respectively) and are the next frontiers.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;ve found this interesting, you can read the full paper &lt;a href=&#34;TODO&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. Above is the introduction (albeit slightly modified), so are many interesting details that weren&amp;rsquo;t covered.&lt;/p&gt;

&lt;h2 id=&#34;example-of-tp-srl-without-a-navigation-skill&#34;&gt;Example of TP-SRL without a navigation skill&lt;/h2&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/1-tp-srl-no-nav.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;h2 id=&#34;examples-of-pick-place-during-training&#34;&gt;Examples of Pick/Place during training&lt;/h2&gt;

&lt;h3 id=&#34;pick&#34;&gt;Pick&lt;/h3&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/3-pick-from-table.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/3-pick-from-fridge.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;h3 id=&#34;place&#34;&gt;Place&lt;/h3&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/4-place-on-table.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/4-place-into-drawer.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;Notice  how much closer the policies are initialized during training than in the video of TP-SRL without a navigation skill.&lt;/p&gt;

&lt;h2 id=&#34;tp-srl-with-a-navigation-skill&#34;&gt;TP-SRL with a Navigation Skill&lt;/h2&gt;

&lt;video height=&#34;100%&#34; width=&#34;100%&#34; controls autoplay&gt;
&lt;source src=&#34;https://wijmans.xyz/files/ver-post/2-tp-srl.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Is Mapping Necessary for Realistic PointGoal Navigation?</title>
      <link>https://wijmans.xyz/publication/mapping-for-pn/</link>
      <pubDate>Sat, 27 Aug 2022 15:05:01 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/mapping-for-pn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI</title>
      <link>https://wijmans.xyz/publication/hm3d/</link>
      <pubDate>Tue, 05 Oct 2021 18:28:47 -0400</pubDate>
      
      <guid>https://wijmans.xyz/publication/hm3d/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
